{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> $\\color{darkslateblue}{\\text{Natural Language Processing}}$</center></h1>\n",
    "\n",
    "<h2><center> $\\color{darkslateblue}{\\text{Theoretical Framework of NPL and its Applications in News Headlines with the LDA Model}}$</center></h2>\n",
    "\n",
    "---\n",
    "\n",
    "<h3><center> $\\color{darkslateblue}{\\text{Antonio Romero Martínez-Eiroa y Beatriz Quevedo Gómez}}$  </center></h3> \n",
    "<h4><center> $\\color{darkslateblue}{\\text{Machine Learning, January 2021}}$  </center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='../data/robot.jpg' > <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field of study focused on making sense of language using statistics and computers. NLP applications include chatbots, translation, sentiment analysis, among many others. The aim of this paper will be to explain and exemplify how it operates.\n",
    "\n",
    "To do so, the streps that will be followed are: \n",
    "\n",
    "1. Understanding the main concepts\n",
    "     * Regular expressions\n",
    "     * Tokenization\n",
    "     * Bag-of-words\n",
    "2. Simple text preprocessing\n",
    "3. Gensim and word vectors\n",
    "     * How to create a gensim dictionary\n",
    "     * TF-IDF with gensim\n",
    "4. Named Entity Recognition\n",
    "     * SpaCy\n",
    "5. Supervised learning with NLP\n",
    "     * Steps\n",
    "6. Implementation of NPL to news headlines\n",
    "     * Data pre-processing\n",
    "     * Bag-of-Words of the data set\n",
    "     * TF-IDF\n",
    "     * LDA\n",
    "     * Performance evaluation by classifying sample document using LDA bag-of-words and TF-IFD models\n",
    "     * Testing model on unseen document\n",
    "7. Conclussions\n",
    "8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{1. Understanding the main concepts}}$</h3>\n",
    "\n",
    "<h4> $\\color{darkslateblue}{\\text{Regular expressions}}$</h4>\n",
    "\n",
    "**Regular expressions** (regex) are strings with a special syntax that allow to match patterns in other strings. They are an excellent tool for text analysis or NLP. Some applications of regular expressions are, for example to find all web links in a document, to parse email addresses or to remove or replace unwanted characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex can be used easily with Python through the **re** library. With `re.match` you can match a substring, this method matches a pattern with a string. It takes the pattern as the fist argument, the string as the second argument and returns the matched object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abc', 'abcdef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use special patterns that regex understands, like the `\\w+`, which will match a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='hi'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_regex = '\\w+'\n",
    "re.match(word_regex, 'hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common regex patterns are:\n",
    "* `\\w+`, matches a word\n",
    "* `\\d` which maches a digit\n",
    "* `\\s` which matches spaces\n",
    "* `.*`, a wildcard that will match any letter or symbol (useful in usernames, for example)\n",
    "* `+` or `*` which allows things to become greedy, grabbing repeats of single letters\n",
    "* `[a-z]` which matches a lowecase group (for instance 'abcdefg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note than while `\\w` will only match a letter, adding the `+` after allows to match a whole word. \n",
    "\n",
    "Additionaly, the patterns used with capital leters such as `\\S`, will do the opposite as the lowercase pattern, matching in this case anything that are not spaces. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's re module can not only match an entire string or substring based on a pattern with `match`, but it can also split a tring on regex (`split`), find all patterns in a string (`findall`) or search for a pattern (`search`). \n",
    "\n",
    "`search` does not require to match the pattern at the beginning of the string, so, to observe the difference between `re.search()` and `re.match()`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `match` and `search` whith the same pattern and string when the pattern is at the beginning of the string, you obtain identical matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('abc', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use `search` for a pattern that appears later in the string, you get the result, but not using `match`. This is because match will try to match a string from the beginning until it cannot match any longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(2, 4), match='cd'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search('cd', 'abcde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.split()` can be used for **tokenization** so you can preprocess text using regex or doing NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Split', 'on', 'spaces']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', 'Split on spaces')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Tokenization}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** is the process of transforming a string or document into tokens (smaller chunks). This is usually one step in the process of preparing a text for NLP.\n",
    "\n",
    "There are many different rules and theories regarding tokenization, and one can create its own rules using regular expressions, but normally tokenization would do things like breaking out words or sentences, separating puntuaction or even just tonkenize parts of a string (linke separating all hashtags in a tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` library is a natural language toolkit and is often used in tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize('Hi there!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why tokenize? It can help whith simple text processing tasks, like mapping part of speech, matching common words, removing unwanted tokens, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in *I don't like Sam's shoes* when it is tokenized (\"I\", \"do\", \"n't\", \"like\", \"Sam\", \"'s\", \"shoes\", \".\") you can clearly see the negation in the `n't` and possesion in the `'s`. This indicators can help to determine meaning from simple texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just tokenizing words, `nltk` has other tokenizers that include:\n",
    "* `sent_tokenize`, which tokenizes a document into individual sentences\n",
    "* `regexp_tokenize`, which tokenizes a string or document based on a regular expression pattern\n",
    "* `TweetTokenizer` is a special class just for tweet tokenization that allows you yo spearate hashtags, mentions and lots of exclamation points!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR is represented using `|` and you can define a group using `()` or a explicit character ranges using `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_digits_and_words = ('(\\d+|\\w+)')\n",
    "re.findall(match_digits_and_words, 'He has 11 cats.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some regex ranges and groups are:\n",
    "\n",
    "* `[A-Za-z]+` : matches upper and lowercase English alphabet\n",
    "* `[0-9]`: numbers from 0 to 9\n",
    "* `[A-Za-z\\-\\.]+` : upper and lowercase English alphabet, - and . (usefull for url websites, for example)\n",
    "* `(a-z)` : a, - and z\n",
    "* `(\\s+|,)` : spaces or a comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 37), match='match lowecase spaces numbers like 12'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = 'match lowecase spaces numbers like 12, but no commas'\n",
    "re.match('[a-z0-9 ]+', my_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Bag-of-words}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of words** is a very simple and basic method for finding topics in a text. For bag of words, you need to first create tokens using tokenization, and then count up all the tokens you have. \n",
    "\n",
    "The theory is that the more frequent a word or token is, the more central or important it might be to the text. Bag of words can be a great way to determine the significant words in a text based on the number of times they are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "counter = Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the word THE appears twice in the bag of words, once with uppercase and once in lowercase. If we added a **preprocessing step**, which will be presented in the following section, to handle this issue, we could lowercase all of the words in the text so each word is counted only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{2. Simple text preprocessing}}$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing helps make for better input data when performing machine learning or other statistical methods, as shown above. Preprocessing steps like tokenization or lowercasing words are commonly used in NLP. \n",
    "\n",
    "Other common techniques are things like **lemmatization or stemming**, where the words are shortened to their root stems, or techniques like **removing stop words**, which are common words in a language that don't carry a lot of meaning (such as and or the, or removing punctuation or unwanted tokens). \n",
    "\n",
    "Since each model and process will have different results, the optimum is to try a few different pre-processing approaches and see which one works best for your task and objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat likes the box. \n",
    "        The box is over the cat.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Firstly we tranform the text all in lowercase using the string lower method. \n",
    "\n",
    "    The string `is_alpha` method will return *True* if the string has only alphabetical characters. We use this method along with an **if** \n",
    "    statement iterating over our tokenized result to only return \n",
    "    **alphabetic strings** (this will effectively strip tokens with numbers \n",
    "    or punctuation). \n",
    "    \n",
    "    To read out the process in both code and English we \n",
    "    say we take each token from the word_tokenize output of the lowercase \n",
    "    text if it contains only alphabetical characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in word_tokenize(text.lower())\n",
    "          if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next line, we use another list comprehension to **remove** words  that are in the stopwords list. This stopwords list for english comes  built in with the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "no_stops = [t for t in tokens\n",
    "            if t not in stopwords.words('english')]\n",
    "\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the example, preprocessing has already improved our bag of words and made it more useful by removing the stopwords and non-alphabetic words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{3. Gensim and word vectors}}$</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gensim** is a popular open-source natural language processing library. It uses top academic models to perform complex tasks like building document or word vectors, corpora (a corpus –or if plural, corpora– is a set of texts used to help implement NLP tasks) and performing topic identification and document comparisons.\n",
    "\n",
    "A **word embedding** or **vector** is trained from a larger corpus and is a multi-dimensional representation of a word or document. \n",
    "\n",
    "With these vectors, we can then see relationships among the words or documents based on how near or far they are and also what similar comparisons we find. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='../data/wordvector.png' > <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in this graphic you can see that the vector operation *king* minus *queen* is approximately equal to *man* minus *woman*. Or that *Spain* is to *Madrid* as *Italy* is to *Rome*. \n",
    "\n",
    "The deep learning algorithm used to create word vectors has been able to distill this meaning based on how those words are used throughout the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of Gensim is the graphic you can see below, which is an example of **LDA visualization**. LDA stands for **latent dirichlet allocation**, and it is a statistical model that can be applied to text using Gensim for topic analysis and modelling. \n",
    "\n",
    "This graph is just a portion of a blog post written in 2015 using Gensim to analyze US presidential addresses. The article is really neat and you can find the link [here](http://tlfvincent.github.io/2015/10/23/presidential-speech-topics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='../data/gensim.png' > <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{How to create a gensim dictionary}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim allows you to build **corpora** and **dictionaries** using simple classes and functions. \n",
    "\n",
    "In the example below the documents are a list of strings that look like movie reviews about space or sci-fi films. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I really liked the movie!',\n",
    "                'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First it is important to do some basic preprocessing. In this case and for brevity we will only tokenize and lowercase, although for better results it would be convenient to apply more, such as removing punctuation and stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) \n",
    "                  for doc in my_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we can pass the tokenized documents to the **Gensim Dictionary** class. This will create a mapping with an id for each token. This is the beginning of our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(tokenized_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now can represent whole documents using just a list of their token ids and how often those tokens appear in each document. We can take a look at the tokens and their ids by looking at the `token2id` attribute, which is a dictionary of all of our tokens and their respective ids in our new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using this dictionary, we can then create a **Gensim corpus**, different from a normal corpus, which is just a collection of documents.\n",
    "\n",
    "    Gensim uses a **simple bag-of-words model** which transforms each document into a bag of words using the token ids and the frequency of each token in the document. In the example we can see that the Gensim corpus is a list of lists: each list item representing one document and each document a series of tuples: \n",
    "    * The first item represents the **tokenid** from the **dictionary** \n",
    "    * The second item represents the token **frequency** in the **document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In so doing, we have a new bag-of-words model and corpus thanks to Gensim. And unlike our previous Counter-based bag of words, this Gensim model can be **easily saved**, **updated** and **reused** thanks to the extra tools we have available in Gensim. \n",
    "\n",
    "Our dictionary can also be updated with new texts and extract only words that meet particular thresholds. We are building a more advanced and feature-rich bag-of-words model which can then be used for future exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Tf-idf with gensim}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-idf**, which stands for *term-frequncy - inverse document frequency*, is a commonly used NLP model that helps you determine the most important words in each document in the corpus. \n",
    "\n",
    "The idea behind *tf-idf* is that each corpus might have more shared words than just stopwords. These common words are like stopwords and should be removed or at least down-weighted in importance. \n",
    "\n",
    "For example, if I am an astronomer, sky might be used often but is not important, so I want to downweight that word. TF-Idf does precisely that. It will take texts that share common language and ensure the most common words across the entire corpus don't show up as keywords. \n",
    "\n",
    "**Tf-idf helps keep the document-specific frequent words weighted high and the common words across the entire corpus weighted low.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation to calculate the weights can be outlined like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='../data/tfidf.png' > <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight of token **i** in document **j** is calculated by taking the term frequency (how many times the token appears in the document) multiplied by the log of the total number of documents divided by the number of documents that contain the same term.\n",
    "\n",
    "The weight ($w_{i,j}$) will be low if the term doesnt appear often in the document because the $tf_{i,j}$ variable will then be low. However, it will also be low if the logarithm is close to zero, meaning the internal equation is low. This way we can see if the total number of documents divded by the number of documents that have the term is close to **one**, then the logarithm will be close to **zero**. So words that occur across many or all documents will have a very low *tf-idf* weight. On the contrary, if the word only occurs in a few documents, that logarithm will return a higher number.\n",
    "\n",
    "A *tf-idf* model can be built using Gensim and the corpus one developed previously. Taking a look at the corpus used in the last example, around movie reviews, the Bag of Words corpus can be used to translate it into a *tf-idf* model by simply passing it in initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.1746298276735174),\n",
       " (7, 0.1746298276735174),\n",
       " (9, 0.1746298276735174),\n",
       " (10, 0.29853166221463673),\n",
       " (11, 0.47316148988815415),\n",
       " (12, 0.7716931521027908)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example one can appreciate how tokens 5, 7 and 9 have a weight of 0.174, whereas token 12 has a weight of 0.77.\n",
    "\n",
    "These weights can help you determine good topics and keywords for a corpus with shared vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{4. Named Entity Recognition}}$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NER** (Named Entity Recognition) is a NLP task used to identify important named entities in the text –such as people, places and organizations– they can even be dates, states, works of art and other categories depending on the libraries and notation you use. \n",
    "\n",
    "NER can be used alongside topic identification, or on its own to determine important items in a text or answer basic natural language understanding questions such as who, what, when and where.\n",
    "\n",
    "Taking the following piece of text, from the English Wikipedia article on *Albert Einstein* you can see the application of the NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src='../data/NER.png' > <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text has been highlighted for different types of named entities that were found using the Stanford NER library. You can see the dates, locations, persons and organizations found and extract infomation on the text based on these named entities.\n",
    "\n",
    "In this way, one can use NER to solve problems like fact extraction as well as which entities are related using computational language models. \n",
    "\n",
    "For example, in this text we can see that Einstein has something to do with the United States, Adolf Hitler and Germany. We can also see by token proximity that Betrand Russel and Einstein created the Russel-Einstein manifesto –all from simple entity highlighting–.\n",
    "\n",
    "NLTK library allows you to interact with named entity recognition via it's own model, but also the aforementioned **Stanford library**. The Stanford library integration requires you to perform a few steps before you can use it, including installing the required Java files and setting system environment variables. You can also use the standford library on its own without integrating it with NLTK or operate it as an API server. \n",
    "\n",
    "The stanford CoreNLP library has great support for named entity recognition as well as some related NLP tasks such as **coreference** (or linking pronouns and entities together) and **dependency trees** to help with parsing meaning and relationships amongst words or phrases in a sentence.\n",
    "\n",
    "For the following simple use case, we will use the built-in named entity recognition with NLTK. To do so, we take a normal sentence, and preprocess it via tokenization. Then, we can tag the sentence for parts of speech. This will add tags for proper nouns, pronouns, adjective, verbs and other part of speech that NLTK uses based on an english grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''In New York, I like to ride the Metro to\n",
    "            visit MOMA and some restaurants rated\n",
    "            well by Ruth Reichl.'''\n",
    "tokenized_sent = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
    "tagged_sent[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take a look at the tags, we see *New* and *York* are tagged *NNP* which is the tag for a **proper noun, singular**.\n",
    "\n",
    "Then we pass this tagged sentence into the `ne_chunk` function, or named entity chunk, which will return the sentence as a **tree**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  I/PRP\n",
      "  like/VBP\n",
      "  to/TO\n",
      "  ride/VB\n",
      "  the/DT\n",
      "  (ORGANIZATION Metro/NNP)\n",
      "  to/TO\n",
      "  visit/VB\n",
      "  (ORGANIZATION MOMA/NNP)\n",
      "  and/CC\n",
      "  some/DT\n",
      "  restaurants/NNS\n",
      "  rated/VBN\n",
      "  well/RB\n",
      "  by/IN\n",
      "  (PERSON Ruth/NNP Reichl/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tree shows the named entities tagged as their own chunks such as GPE or **geopolitical entity** for *New York*, or *MOMA* and *Metro* as **organizations**. It also identifies *Ruth Reichl* as a **person**. \n",
    "\n",
    "It does so without consulting a knowledge base, like wikipedia, but instead uses **trained statistical and grammatical parsers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{SpaCy}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpaCy** is a NLP library similar to **Gensim** but with different implementations, including a particular focus on **creating NLP pipelines to generate models and corpora**. SpaCy is open-source and has several extra libraries and tools built by the same team, including **Displacy** - a visualization tool for viewing parse trees which uses Node-js to create interactive text.\n",
    "\n",
    "Using the **displacy entity recognition visualizer**, we can enter the sentence used in the last example:\n",
    "\n",
    "<center> <img src='../data/SpaCy.png' > <center>\n",
    "    \n",
    "Here, we can see the SpaCy has identified three named entities and tagged them with the appropriate entity label –such as location or person–. SpaCy also has tools to build word and document vectors from text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To start using spacy for NER, we must first install it and download all the appropriate pre-trained word vectors (you can also train vectors yourself and load them; but the pretrained ones let us get started immediately). \n",
    "\n",
    "    We can load those into an object, `nlp`, which functions similarly to our Gensim dictionary and corpus. It has several linked objects, including `entity` which is an Entity Recognizer object from the pipeline module. This is what is used to **find entities in the text**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.pipes.EntityRecognizer at 0x7fa6615aba60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nlp.entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we load a new document by passing a string into the NLP variable. When the document is loaded, the named entities are stored as a document attribute called `ents`. We see Spacy properly tagged and identified the three main entities in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Berlin, Germany, Angela Merkel)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"\"\"Berlin is the capital of Germany;\n",
    "            and the residence of Chancellor Angela Merkel.\"\"\")\n",
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also investigate the labels of each entity by using indexing to pick out the first entity and the `label_` attribute to see the label for that particular entity. \n",
    "\n",
    "  Here we see the label for Berlin is GPE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin GPE\n"
     ]
    }
   ],
   "source": [
    "print(doc.ents[0], doc.ents[0].label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has several other language models available, including advanced German and Chinese implementations. It's a great tool especially if you want to build your own extraction and NLP pipeline quickly and iteratively.\n",
    "\n",
    "**Why use Spacy for NER?** Outside of being able to integrate with the other great Spacy features like easy pipeline creation, it has a different set of entity types and often labels entities differently than nltk. In addition, Spacy comes with informal language corpora, allowing you to more easily find entities in documents like Tweets and chat messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{5. Supervised learning with NLP}}$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning** is a form of machine learning where you are given or create training data. This data has a label or outcome which you want the model or algorithm to learn.\n",
    "\n",
    "To help create features and train a model, we will use `Scikit learn`, a powerful open-source library. One of the ways you can create supervised learning data from text is by using bag of words models or TFIDF as features.\n",
    "\n",
    "Let's say you have a dataset full of movie plots and genres from the IMDB database, as shown in the following chart:\n",
    "\n",
    "<center> <img src='../data/IMDB.png' > <center>\n",
    "\n",
    "\n",
    "Action and Sci-Fi movies have been separated, removing any movies labeled both action and Sci-Fi.\n",
    "    \n",
    "You want to **predict** whether a movie is action or sci-fi based on the plot summary. The dataset we've extracted has categorical features generated using some preprocessing. We can see the plot summary, and the sci-fi and action columns. You can also see the Sci-Fi column, which is 1 for movies that are scifi and 0 for movies that are action. The Action column is the inverse of the Sci-Fi column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Steps}}$</h4>\n",
    "\n",
    "The superivsed learning process is:\n",
    "1. Collection and preprocessing of data. \n",
    "2. Determine a label –what we want the model to learn–. \n",
    "3. Split the data into training and testing datasets, keeping them separate so we can build our model using only the training data. The test data remains unseen so we can test how well our model performs after it is trained. **This is an essential part of Supervised Learning**\n",
    "4. Extract features from the text to predict the label. We will use a bagof words vectorizer built into scikit-learn to do so. \n",
    "5. After the model is trained, we can then test it using the test dataset. There are also other methods to evaluate model performance, such as k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{6. Implementation of NLP to news headlines}}$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the basis of the **NLP** has been established, is time to apply it to an example. \n",
    "\n",
    "The aim is to implement **LDA** to a data set that contains data of news headlines published over a period of seventeen years and split them into topics, to see if the model can classify correctly a headline into a predefined topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Data load}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to import `pandas` and the data that will be used. The DataFrame is sourced from the **Australian news source ABC** ([Australian Broadcasting Corporation](https://www.abc.net.au/)).\n",
    "\n",
    "In order to be able to work better with the data, two columns will be set up: `headline_text` –with the headline of the article in Ascii , English , lowercase– and `index` –the position the headline occupies in the DataFrame–. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/abcnews-date-text.csv', error_bad_lines = False);\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception to be raised, and no DataFrame will be returned. If `error_bad_lines` is set as *False*, then these “bad lines” will dropped from the DataFrame that is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the DataFrame is renamed as `documents` and it is observed that it has a total of **1,186,018 headlines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186018"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DF therefore has the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ambitious olsson wins triple jump</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>antic delighted with record breaking barca</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aussie qualifier stosur wastes four memphis match</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aust addresses un security council over iraq</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>australia is locked into war timetable opp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4\n",
       "5                  ambitious olsson wins triple jump      5\n",
       "6         antic delighted with record breaking barca      6\n",
       "7  aussie qualifier stosur wastes four memphis match      7\n",
       "8       aust addresses un security council over iraq      8\n",
       "9         australia is locked into war timetable opp      9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Data preprocessing}}$</h4>\n",
    "\n",
    "Once the DF is loaded and examined, the first step of the Natural Language Processing, **preprocessing** will be carried out. \n",
    "\n",
    "In this section it will be done the **tokenization**, splitting the text into sentences and then into words, as well as the removal of the punctuation. Words that have fewer than 3 characters and all **stopwords** will also be removed, but **lowercasing** wont be necessary as the words are already in that format.\n",
    "\n",
    "Likewise, words will be **lemmatized** (words in third person are changed to first person and verbs in past and future tenses are changed into present) and **stemmed** (reduced to their root form). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be done through the libraries `gensim` and `nltk` wich will be imported along with its **dependencies** and `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtst the lemmatizing and stemming will be done. An example of how they work can be seen below.\n",
    "\n",
    "Using `WordNetLemmatizer` the verb *went* in past tense is transformed to *go*. The parameter `pos = 'v'` is pointing out that it is a verb. However, it can also be a noun (n), an adjetive (j) or an adverb (r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through `SnowballStemmer` one can choose the language, so a group of original words are return into their root. Then the words will be transformed from plural to singular and the function `stemmer.stem` will be applied to them so they are stemmed, creating a DF with the words before and after being processed throughout the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "\n",
    "stem = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': stem})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So once the functionality of both lemmatize and stemming has been visually seen, we will proceed to define the functions.\n",
    "\n",
    "The *first* function is defined to get the words lemmatized and stemmed at the same time. \n",
    "\n",
    "The *second* function is in charge of the tokenization once the previous process it's been done, which is the reason why the first fuction is contained by the second. In this function the **stopwords** and the words with a lenght of less than **3** charcters are ruled out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, a single headline (the one in the line `3,000` for example) has been selected so the diference between the original text, splited by blank spaces, and the the text once it has been processed by the fuction descibed above can be seen.\n",
    "\n",
    "It is also splitted by spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['brogden', 'pledges', 'cut', 'in', 'hospital', 'waiting', 'lists']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['brogden', 'pledg', 'hospit', 'wait', 'list']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 3000].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the function is applied to the hole set of headlines, and the first ten headlines of the resulting data frame are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [decid, communiti, broadcast, licenc]\n",
       "1                               [wit, awar, defam]\n",
       "2           [call, infrastructur, protect, summit]\n",
       "3                      [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travel]\n",
       "5               [ambiti, olsson, win, tripl, jump]\n",
       "6           [antic, delight, record, break, barca]\n",
       "7    [aussi, qualifi, stosur, wast, memphi, match]\n",
       "8            [aust, address, secur, council, iraq]\n",
       "9                         [australia, lock, timet]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for example, on the second headline we can see the topic is been talked about is probably about a *witness* that is aware of *defamation*. Another example is in the fifth headline, where the headline has something to do with an *strike* that can affect *australian travel*.\n",
    "\n",
    "In this way the pre-processing section is finished, in which the necessary changes to the text have been made and it is ready to be worked on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Bag-of-words of the data set}}$</h4>\n",
    "\n",
    "In this section a dictionary will be formed from `processed_docs`, containing the number of times a word appears in the training set. This one of the methods for finding topics in a text that we will see. The other one is TF-IDF, which will be explained later.\n",
    "\n",
    "The `corpora.Dictionary` module is applied to the processed document. It implements the concept of a Dictionary – a mapping between words and their integer ids–. The dictionary object is used to create a bag of words corpus which will be used later on the input to topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the dictionary counts all the words in the document by iterating through it. The dictionary contains the number of times a word appears in the document.\n",
    "\n",
    "The result of the function below is the word and its order. Note that count stops at 10 because is the limit of words a topic can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 communiti\n",
      "2 decid\n",
      "3 licenc\n",
      "4 awar\n",
      "5 defam\n",
      "6 wit\n",
      "7 call\n",
      "8 infrastructur\n",
      "9 protect\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function from Gensim `filter_extremes` has de functionality of filtering out tokens that apperar in **less than 15 documents** or **more than the 50%** of them in order to keep only the first 100,000 most frequent tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, `doc2bow`, a function from also Gensim,  creates (for each document) a dictionary reporting how **many words** and **how many times** those words appear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(365, 1), (393, 1), (665, 1), (1142, 1), (1191, 1)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to the headline that has ben chosen as example (#3000), the results show the word number 365 appears one time, so as it happens with the following words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **preview bag-of-words for the sample preprocessed document** is the result of the following function, which is the same result as the dictionary function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 365 (\"hospit\") appears 1 time.\n",
      "Word 393 (\"pledg\") appears 1 time.\n",
      "Word 665 (\"wait\") appears 1 time.\n",
      "Word 1142 (\"list\") appears 1 time.\n",
      "Word 1191 (\"brogden\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_3000 = bow_corpus[3000]\n",
    "\n",
    "for i in range(len(bow_doc_3000)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_3000[i][0], \n",
    "                                                     dictionary[bow_doc_3000[i][0]], \n",
    "                                                     bow_doc_3000[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keep in mind that when it comes to headlines, it is normal that the words are not repeated*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{TF-IDF}}$</h4>\n",
    "\n",
    "In this section a **TF-IDF** (*Term-Frequency - Inverse Document Frequency*) model will be created using `models.TfidfModel` on `bow_corpus` and saved to `tfidf`, so that afterwards it can be applied the transformation to the entire corpus and call it `corpus_tfidf`. \n",
    "\n",
    "By doing this, the number of times each word appears in a headline will be counted, measured by the number of time the word appears in the corpus. It is asumed that words that appears a lot in the hole corpus are less informative. \n",
    "\n",
    "The result of this process will be a **TF-IDF score** (its weight) for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5850076620505259),\n",
      " (1, 0.38947256567331934),\n",
      " (2, 0.4997099083387053),\n",
      " (3, 0.5063271308533074)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{LDA}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will run **LDA** using both *bag-of-words* and *TF-IDF* models so their results can be compared.\n",
    "\n",
    "As mentioned earlier, LDA stands for **Latent Dirichlet Allocation**, a statistical model that can be applied to text using Gensim for topic analysis and modeling. \n",
    "\n",
    "In this way, LDA will show the relative weight of each word for several topics in both BOW and TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Running LDA using Bag of Words**\n",
    "\n",
    "`lda_model` is applied to the bag of words created before.\n",
    "\n",
    "The parameter `num_topics` indicates the number of requested latent topics to be extracted from the training corpus (10 in our case), `id2word` is used to determine the vocabulary size, as well as for debugging and topic printing, `pases` is the number of passes through the entire corpus (2) and `workers` indicates the number of workers processes to be used for parallelization (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics = 10, \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this in mind, the topics are generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.022*\"hous\" + 0.022*\"south\" + 0.020*\"north\" + 0.017*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.012*\"west\" + 0.011*\"hospit\" + 0.011*\"coast\" + 0.010*\"investig\"\n",
      "Topic: 1 \n",
      "Words: 0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"protest\" + 0.020*\"dead\" + 0.019*\"polic\" + 0.019*\"attack\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"michael\"\n",
      "Topic: 2 \n",
      "Words: 0.056*\"australia\" + 0.045*\"australian\" + 0.026*\"world\" + 0.017*\"canberra\" + 0.017*\"test\" + 0.013*\"win\" + 0.011*\"final\" + 0.011*\"farm\" + 0.011*\"open\" + 0.010*\"return\"\n",
      "Topic: 3 \n",
      "Words: 0.030*\"polic\" + 0.029*\"charg\" + 0.026*\"court\" + 0.024*\"death\" + 0.024*\"murder\" + 0.020*\"woman\" + 0.017*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\" + 0.013*\"trial\"\n",
      "Topic: 4 \n",
      "Words: 0.019*\"chang\" + 0.018*\"say\" + 0.015*\"speak\" + 0.015*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.012*\"concern\" + 0.011*\"flood\" + 0.011*\"fear\" + 0.010*\"emerg\"\n",
      "Topic: 5 \n",
      "Words: 0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"price\" + 0.012*\"lose\" + 0.012*\"break\"\n",
      "Topic: 6 \n",
      "Words: 0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"leader\" + 0.011*\"parti\" + 0.010*\"campaign\"\n",
      "Topic: 7 \n",
      "Words: 0.020*\"donald\" + 0.014*\"nation\" + 0.014*\"farmer\" + 0.013*\"rural\" + 0.013*\"time\" + 0.013*\"council\" + 0.012*\"indigen\" + 0.012*\"school\" + 0.011*\"plan\" + 0.011*\"commiss\"\n",
      "Topic: 8 \n",
      "Words: 0.044*\"trump\" + 0.037*\"year\" + 0.035*\"sydney\" + 0.028*\"queensland\" + 0.022*\"home\" + 0.021*\"adelaid\" + 0.018*\"perth\" + 0.016*\"brisban\" + 0.015*\"peopl\" + 0.015*\"royal\"\n",
      "Topic: 9 \n",
      "Words: 0.031*\"govern\" + 0.018*\"feder\" + 0.016*\"warn\" + 0.015*\"countri\" + 0.015*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.013*\"say\" + 0.012*\"stori\" + 0.011*\"health\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can se that for example, **topic 1** has words such as *kill, shoot, dead, attack, assault* which indicates the topic is about crimes and offenses. **Topic 4** has words like *change, climat, concern, flood, fear, emergency*, so basically climate change, and **topic 6**, *elec, state, labor, liber, leader, parti, campaign*, which it appears to be a topic about elections and the different parties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Running LDA using TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the corpus created before using TF-IDF, the same model will be applied. \n",
    "\n",
    "The only parameter changing is workers beacuse this process require more computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, \n",
    "                                             num_topics=10,\n",
    "                                             id2word = dictionary,\n",
    "                                             passes=2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.020*\"countri\" + 0.015*\"hour\" + 0.008*\"christma\" + 0.007*\"andrew\" + 0.007*\"insid\" + 0.006*\"august\" + 0.006*\"june\" + 0.006*\"bushfir\" + 0.006*\"brief\" + 0.005*\"kill\"\n",
      "Topic: 1 Word: 0.011*\"elect\" + 0.007*\"sport\" + 0.007*\"liber\" + 0.006*\"financ\" + 0.006*\"mark\" + 0.006*\"jam\" + 0.005*\"marriag\" + 0.005*\"quiz\" + 0.005*\"asylum\" + 0.005*\"toni\"\n",
      "Topic: 2 Word: 0.008*\"street\" + 0.007*\"live\" + 0.007*\"girl\" + 0.006*\"octob\" + 0.006*\"polit\" + 0.005*\"foreign\" + 0.005*\"refuge\" + 0.005*\"senat\" + 0.005*\"univers\" + 0.005*\"blog\"\n",
      "Topic: 3 Word: 0.021*\"news\" + 0.019*\"market\" + 0.015*\"rural\" + 0.010*\"price\" + 0.009*\"drought\" + 0.008*\"turnbul\" + 0.008*\"farmer\" + 0.008*\"share\" + 0.008*\"farm\" + 0.007*\"nation\"\n",
      "Topic: 4 Word: 0.022*\"trump\" + 0.010*\"drum\" + 0.010*\"govern\" + 0.009*\"health\" + 0.007*\"tuesday\" + 0.006*\"michael\" + 0.006*\"tasmania\" + 0.006*\"fund\" + 0.006*\"david\" + 0.006*\"care\"\n",
      "Topic: 5 Word: 0.016*\"crash\" + 0.009*\"miss\" + 0.009*\"search\" + 0.008*\"die\" + 0.008*\"monday\" + 0.008*\"road\" + 0.007*\"fatal\" + 0.007*\"polic\" + 0.007*\"morrison\" + 0.007*\"grandstand\"\n",
      "Topic: 6 Word: 0.011*\"climat\" + 0.009*\"hobart\" + 0.009*\"chang\" + 0.007*\"cattl\" + 0.006*\"northern\" + 0.006*\"territori\" + 0.006*\"decemb\" + 0.005*\"remot\" + 0.005*\"australia\" + 0.005*\"princ\"\n",
      "Topic: 7 Word: 0.019*\"charg\" + 0.017*\"murder\" + 0.016*\"polic\" + 0.013*\"donald\" + 0.012*\"alleg\" + 0.012*\"court\" + 0.010*\"jail\" + 0.010*\"arrest\" + 0.010*\"woman\" + 0.009*\"shoot\"\n",
      "Topic: 8 Word: 0.011*\"interview\" + 0.009*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"friday\" + 0.007*\"wednesday\" + 0.006*\"scott\" + 0.006*\"thursday\" + 0.006*\"cricket\" + 0.006*\"leagu\"\n",
      "Topic: 9 Word: 0.013*\"stori\" + 0.011*\"weather\" + 0.008*\"rat\" + 0.008*\"peter\" + 0.007*\"rugbi\" + 0.006*\"syria\" + 0.006*\"brexit\" + 0.006*\"explain\" + 0.006*\"facebook\" + 0.005*\"father\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case topics are different, for example the **topic 8** seems to be about cricket league final in australia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the two models have been done, the main diference between *BOW* and *TF-IDF* is BOW just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the importancy of the words as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Performance evaluation by classifying sample document using LDA bag-of-words model}}$</h4>\n",
    "\n",
    "\n",
    "We will check where our test document would be classified. Lets remind what are the main words from the headline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brogden', 'pledg', 'hospit', 'wait', 'list']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function indicates the score of each topic and the relative value in descendinG order thanks to `sorted(key=lambda tup: -1*tup[1]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.35019564628601074\t \n",
      "Topic: 0.044*\"trump\" + 0.037*\"year\" + 0.035*\"sydney\" + 0.028*\"queensland\" + 0.022*\"home\" + 0.021*\"adelaid\" + 0.018*\"perth\" + 0.016*\"brisban\" + 0.015*\"peopl\" + 0.015*\"royal\"\n",
      "\n",
      "Score: 0.34957143664360046\t \n",
      "Topic: 0.031*\"govern\" + 0.018*\"feder\" + 0.016*\"warn\" + 0.015*\"countri\" + 0.015*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.013*\"say\" + 0.012*\"stori\" + 0.011*\"health\"\n",
      "\n",
      "Score: 0.18346960842609406\t \n",
      "Topic: 0.022*\"hous\" + 0.022*\"south\" + 0.020*\"north\" + 0.017*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.012*\"west\" + 0.011*\"hospit\" + 0.011*\"coast\" + 0.010*\"investig\"\n",
      "\n",
      "Score: 0.016682324931025505\t \n",
      "Topic: 0.019*\"chang\" + 0.018*\"say\" + 0.015*\"speak\" + 0.015*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.012*\"concern\" + 0.011*\"flood\" + 0.011*\"fear\" + 0.010*\"emerg\"\n",
      "\n",
      "Score: 0.016682039946317673\t \n",
      "Topic: 0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"leader\" + 0.011*\"parti\" + 0.010*\"campaign\"\n",
      "\n",
      "Score: 0.016681233420968056\t \n",
      "Topic: 0.020*\"donald\" + 0.014*\"nation\" + 0.014*\"farmer\" + 0.013*\"rural\" + 0.013*\"time\" + 0.013*\"council\" + 0.012*\"indigen\" + 0.012*\"school\" + 0.011*\"plan\" + 0.011*\"commiss\"\n",
      "\n",
      "Score: 0.0166794303804636\t \n",
      "Topic: 0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"protest\" + 0.020*\"dead\" + 0.019*\"polic\" + 0.019*\"attack\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"michael\"\n",
      "\n",
      "Score: 0.0166794303804636\t \n",
      "Topic: 0.056*\"australia\" + 0.045*\"australian\" + 0.026*\"world\" + 0.017*\"canberra\" + 0.017*\"test\" + 0.013*\"win\" + 0.011*\"final\" + 0.011*\"farm\" + 0.011*\"open\" + 0.010*\"return\"\n",
      "\n",
      "Score: 0.0166794303804636\t \n",
      "Topic: 0.030*\"polic\" + 0.029*\"charg\" + 0.026*\"court\" + 0.024*\"death\" + 0.024*\"murder\" + 0.020*\"woman\" + 0.017*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\" + 0.013*\"trial\"\n",
      "\n",
      "Score: 0.0166794303804636\t \n",
      "Topic: 0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"price\" + 0.012*\"lose\" + 0.012*\"break\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[3000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test headline (no.3000) has the highest probability to be part of the topic that the LDA model for BOW assigned on the first place, which talks about different states of Australia (Sydney, Queensland, Perth, Brisbane...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Performance evaluation by classifying sample document using LDA TF-IDF model}}$</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.40001052618026733\t \n",
      "Topic: 0.011*\"elect\" + 0.007*\"sport\" + 0.007*\"liber\" + 0.006*\"financ\" + 0.006*\"mark\" + 0.006*\"jam\" + 0.005*\"marriag\" + 0.005*\"quiz\" + 0.005*\"asylum\" + 0.005*\"toni\"\n",
      "\n",
      "Score: 0.2735215127468109\t \n",
      "Topic: 0.022*\"trump\" + 0.010*\"drum\" + 0.010*\"govern\" + 0.009*\"health\" + 0.007*\"tuesday\" + 0.006*\"michael\" + 0.006*\"tasmania\" + 0.006*\"fund\" + 0.006*\"david\" + 0.006*\"care\"\n",
      "\n",
      "Score: 0.20962771773338318\t \n",
      "Topic: 0.019*\"charg\" + 0.017*\"murder\" + 0.016*\"polic\" + 0.013*\"donald\" + 0.012*\"alleg\" + 0.012*\"court\" + 0.010*\"jail\" + 0.010*\"arrest\" + 0.010*\"woman\" + 0.009*\"shoot\"\n",
      "\n",
      "Score: 0.016692066565155983\t \n",
      "Topic: 0.011*\"climat\" + 0.009*\"hobart\" + 0.009*\"chang\" + 0.007*\"cattl\" + 0.006*\"northern\" + 0.006*\"territori\" + 0.006*\"decemb\" + 0.005*\"remot\" + 0.005*\"australia\" + 0.005*\"princ\"\n",
      "\n",
      "Score: 0.016692031174898148\t \n",
      "Topic: 0.008*\"street\" + 0.007*\"live\" + 0.007*\"girl\" + 0.006*\"octob\" + 0.006*\"polit\" + 0.005*\"foreign\" + 0.005*\"refuge\" + 0.005*\"senat\" + 0.005*\"univers\" + 0.005*\"blog\"\n",
      "\n",
      "Score: 0.01669188216328621\t \n",
      "Topic: 0.016*\"crash\" + 0.009*\"miss\" + 0.009*\"search\" + 0.008*\"die\" + 0.008*\"monday\" + 0.008*\"road\" + 0.007*\"fatal\" + 0.007*\"polic\" + 0.007*\"morrison\" + 0.007*\"grandstand\"\n",
      "\n",
      "Score: 0.01669144071638584\t \n",
      "Topic: 0.020*\"countri\" + 0.015*\"hour\" + 0.008*\"christma\" + 0.007*\"andrew\" + 0.007*\"insid\" + 0.006*\"august\" + 0.006*\"june\" + 0.006*\"bushfir\" + 0.006*\"brief\" + 0.005*\"kill\"\n",
      "\n",
      "Score: 0.01669127307832241\t \n",
      "Topic: 0.013*\"stori\" + 0.011*\"weather\" + 0.008*\"rat\" + 0.008*\"peter\" + 0.007*\"rugbi\" + 0.006*\"syria\" + 0.006*\"brexit\" + 0.006*\"explain\" + 0.006*\"facebook\" + 0.005*\"father\"\n",
      "\n",
      "Score: 0.016691159456968307\t \n",
      "Topic: 0.021*\"news\" + 0.019*\"market\" + 0.015*\"rural\" + 0.010*\"price\" + 0.009*\"drought\" + 0.008*\"turnbul\" + 0.008*\"farmer\" + 0.008*\"share\" + 0.008*\"farm\" + 0.007*\"nation\"\n",
      "\n",
      "Score: 0.016690367832779884\t \n",
      "Topic: 0.011*\"interview\" + 0.009*\"australia\" + 0.009*\"world\" + 0.009*\"final\" + 0.007*\"friday\" + 0.007*\"wednesday\" + 0.006*\"scott\" + 0.006*\"thursday\" + 0.006*\"cricket\" + 0.006*\"leagu\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[3000]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test headline (no.3000) has the highest probability to be part of the topic that the LDA model for TF-IDF assigned on the first place, which talks about elections, finance, marks... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> $\\color{darkslateblue}{\\text{Testing model on unseen document}}$</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, a new headline is defined and saved as `unseen_document`. The headline will be *How a Pentagon deal became an identity crisis for Google*\n",
    "\n",
    "This headline is **preprocessed** por lemmatization, stemming and tokenization as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.48979589343070984\t Topic: 0.021*\"market\" + 0.020*\"news\" + 0.018*\"women\" + 0.018*\"live\" + 0.016*\"tasmania\" + 0.013*\"high\" + 0.013*\"rise\" + 0.012*\"price\" + 0.012*\"lose\" + 0.012*\"break\" + 0.012*\"street\" + 0.011*\"fall\" + 0.011*\"gold\" + 0.011*\"record\" + 0.010*\"busi\"\n",
      "Score: 0.21052519977092743\t Topic: 0.036*\"elect\" + 0.018*\"water\" + 0.018*\"state\" + 0.016*\"tasmanian\" + 0.012*\"labor\" + 0.011*\"liber\" + 0.011*\"morrison\" + 0.011*\"leader\" + 0.011*\"parti\" + 0.010*\"campaign\" + 0.010*\"give\" + 0.009*\"green\" + 0.009*\"season\" + 0.009*\"futur\" + 0.008*\"talk\"\n",
      "Score: 0.1829420030117035\t Topic: 0.030*\"polic\" + 0.029*\"charg\" + 0.026*\"court\" + 0.024*\"death\" + 0.024*\"murder\" + 0.020*\"woman\" + 0.017*\"face\" + 0.017*\"alleg\" + 0.016*\"crash\" + 0.013*\"trial\" + 0.012*\"jail\" + 0.012*\"accus\" + 0.011*\"case\" + 0.011*\"guilti\" + 0.011*\"victoria\"\n",
      "Score: 0.01667730137705803\t Topic: 0.031*\"govern\" + 0.018*\"feder\" + 0.016*\"warn\" + 0.015*\"countri\" + 0.015*\"fund\" + 0.014*\"claim\" + 0.014*\"life\" + 0.013*\"say\" + 0.012*\"stori\" + 0.011*\"health\" + 0.011*\"scott\" + 0.011*\"presid\" + 0.011*\"make\" + 0.011*\"budget\" + 0.010*\"reveal\"\n",
      "Score: 0.016677286475896835\t Topic: 0.019*\"chang\" + 0.018*\"say\" + 0.015*\"speak\" + 0.015*\"power\" + 0.013*\"worker\" + 0.012*\"climat\" + 0.012*\"concern\" + 0.011*\"flood\" + 0.011*\"fear\" + 0.010*\"emerg\" + 0.010*\"storm\" + 0.010*\"resid\" + 0.010*\"minist\" + 0.009*\"station\" + 0.009*\"meet\"\n",
      "Score: 0.016676461324095726\t Topic: 0.022*\"hous\" + 0.022*\"south\" + 0.020*\"north\" + 0.017*\"bushfir\" + 0.016*\"miss\" + 0.013*\"interview\" + 0.012*\"west\" + 0.011*\"hospit\" + 0.011*\"coast\" + 0.010*\"investig\" + 0.010*\"search\" + 0.010*\"island\" + 0.010*\"find\" + 0.009*\"christma\" + 0.009*\"rescu\"\n",
      "Score: 0.016676461324095726\t Topic: 0.031*\"kill\" + 0.023*\"shoot\" + 0.021*\"protest\" + 0.020*\"dead\" + 0.019*\"polic\" + 0.019*\"attack\" + 0.014*\"offic\" + 0.013*\"assault\" + 0.013*\"chines\" + 0.011*\"michael\" + 0.011*\"parliament\" + 0.011*\"bank\" + 0.010*\"sexual\" + 0.010*\"say\" + 0.010*\"free\"\n",
      "Score: 0.016676461324095726\t Topic: 0.056*\"australia\" + 0.045*\"australian\" + 0.026*\"world\" + 0.017*\"canberra\" + 0.017*\"test\" + 0.013*\"win\" + 0.011*\"final\" + 0.011*\"farm\" + 0.011*\"open\" + 0.010*\"return\" + 0.009*\"beat\" + 0.009*\"hobart\" + 0.008*\"cricket\" + 0.007*\"right\" + 0.007*\"race\"\n",
      "Score: 0.016676461324095726\t Topic: 0.020*\"donald\" + 0.014*\"nation\" + 0.014*\"farmer\" + 0.013*\"rural\" + 0.013*\"time\" + 0.013*\"council\" + 0.012*\"indigen\" + 0.012*\"school\" + 0.011*\"plan\" + 0.011*\"commiss\" + 0.010*\"help\" + 0.010*\"drum\" + 0.010*\"communiti\" + 0.010*\"industri\" + 0.010*\"park\"\n",
      "Score: 0.016676461324095726\t Topic: 0.044*\"trump\" + 0.037*\"year\" + 0.035*\"sydney\" + 0.028*\"queensland\" + 0.022*\"home\" + 0.021*\"adelaid\" + 0.018*\"perth\" + 0.016*\"brisban\" + 0.015*\"peopl\" + 0.015*\"royal\" + 0.012*\"abus\" + 0.012*\"game\" + 0.012*\"leav\" + 0.011*\"darwin\" + 0.010*\"babi\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model works! \n",
    "\n",
    "It is asigning the headline to a topic that talks about **the market, news, price, lose, break (from \"breaking news\" probably), fall and business**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{7. Conclussions}}$</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP** is a tool that can encompass great things whose applications are many. In this work a theoretical base has been presented, on which the knowledge of the practical part is based. \n",
    "\n",
    "There, a **LDA** model is created that assigns a topic to a specific headline. \n",
    "\n",
    "As it can be seen in the results, the LDA model difers when implemented using **Bag-Of-Words** as input or **TF-IDF**. This is due to the diferences between both algotims, the fist one is simpler and only counts words while the second one is more complex, giving each a word a certain weight considering it in each headline a in the hole document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> $\\color{darkslateblue}{\\text{8. References}}$</h3>\n",
    "\n",
    "* http://www.nltk.org/book/\n",
    "* https://www.youtube.com/watch?v=X4d4MiTVNcw&t=475s&utm_content=145787035&utm_medium=social&utm_source=linkedin&hss_channel=lcp-3740012\n",
    "* https://medium.com/towards-artificial-intelligence/natural-language-processing-with-spacy-steps-and-examples-155618e84103\n",
    "* https://code.datasciencedojo.com/datasciencedojo/tutorials/blob/master/Introduction%20to%20Natural%20Language%20Processing/Introduction%20to%20Natural%20Language%20Processing.pdf?utm_content=142114660&utm_medium=social&utm_source=linkedin&hss_channel=lcp-3740012\n",
    "* https://www.iotcentral.io/blog/see-this-simple-introduction-to-natural-language-processing-nlp\n",
    "* https://www.datasciencecentral.com/profiles/blogs/analyzing-the-structure-and-effectiveness-of-news-headlines-using\n",
    "* https://www.datasciencecentral.com/profiles/blogs/how-i-used-nlp-spacy-to-screen-data-science-resumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
